# Assignment 3: PINN

## 学生信息
- 姓名：曹以楷
- 学号：2400011486

## Warning

`python`自带的`pickle`库，在读取文件复原时，实际上调用的是`__reduce__`方法，也就是一个可以自定义的，可以运行任意代码的方法，这是一个不安全的行为。而`pytorch`的`save`和`load`方法，实际上调用的还是`pickle`库，仍然是不安全的，所以不应该使用`pinn.pth`的方式来保存训练结果。

## 运行方式

1. 修改`train.py`中的`main`函数，选择`run`或者`exp`模式。
2. 打开`train.sh`，根据当前GPU数，配置运行GPU。
2. 在`assignment3`文件夹下，运行`./train.sh`。

## 模型实现

### 模型结构

**问题描述：** 在立方体区域 $[-1,1]^3$ 中求解泊松方程
$$\nabla^2 \phi = -\rho(x,y,z)$$
其中电荷密度为 $\rho(x,y,z) = 100xyz^2$，边界条件为 $\phi = 0$。

为了求解这个问题，课程网站给出的损失函数为
$$\mathcal{L} = \mathcal{L}_{\text{boundary}} + \beta \cdot \mathcal{L}_{\text{PDE}}$$
如果在任意边界的形状下，我们只能通过Loss来约束PINN。但在这样一种非常好看的边界条件，我们可以令
$$\phi(x, y, z)=(1-x^2)(1-y^2)(1-z^2)u(x, y, z)$$
直接满足边界条件，而我们利用PINN来拟合$u(x, y, z)$即可。此时我们的损失函数为
$$\mathcal{L} = \mathcal{L}_{\text{PDE}}=\overline{(\nabla^2\phi+\rho)^2}$$

激活函数方面，由于我们需要求二阶导，而ReLU等激活函数不适用这种情况，采取`tanh`作为激活函数。而优化器选择使用了经典的`Adam`，这也导致了我们对于学习率的选择为`0.001`，具体超参数选择我们在之后的内容中详细阐述。

### 训练加速

为了加快训练速度，充分利用服务器中10张4080显卡，我使用了`accelerate`库来进行多GPU训练，其背后使用的是`pytorch`内置的分布式数据并行方式（Distributed Data Parallel, DDP）。

具体来说，单卡训练时，每次在$[-1, 1]^3$中的采样数量为`sample_num`，每次训练`num_epochs`个轮次。而使用`accelerate`在$N$张卡上训练时，会独立的复制一个模型，在每个GPU中采样`sample_num`个点，训练后同步到每个GPU，同样训练`num_epochs`个轮次。那么，在付出进程间通信的时间成本后，等效训练的采样数量大大增加，同时充分调用了每张卡的算力，减少了单卡的内存占用。

根据测试，使用5张GPU时，由于进程间通信，耗时大约比单卡训练增加$40\%\sim60\%$，然而等效的`sample_num`是单卡训练的5倍。

### 不同超参数多次训练

为了得到一个比较好的超参数选择，我们需要测试不同超参数配置的情况下，模型收敛的结果。为了避免复杂的进程以及内存管理，我采取了`accelerate`来统一管理，即在同一个`Accelerator`下创建多个不同超参数的`PoissonTrainer`进行训练。利用
```python
del trainer.model, trainer.optimizer
torch.cuda.empty_cache()
```
在每次训练后释放部分占用的显存。虽然并未完全释放干净，部分如NCCL之类的模块尚未清除。但在5GPU的情况下，每个GPU仅占用600MiB左右。

## 训练结果

### $\phi=\phi(x,y,z)$

![看起来很对啊](./data/phi.png)

### 超参数选择

我们在固定3层隐藏层，`num_epochs`=2000的情况下，调整`learning_rate`和`sample_num`，Loss曲线的图像均保存在`/home/stu2400011486/assignments/assignment3/data`文件夹下。

#### 学习率

对于learning_rate=0.005的情况，可以看到其收敛后，loss不会保持收敛，会突然增大。这是因为Adam优化器为了避免陷入局部最优解，其步长可以写为
$$
\Delta\theta_t=-\eta\frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon}
$$
故在收敛点附近，在$v_t$衰减较快的情况下，$\Delta\theta$可能过大，导致步长过长，突然跃至较远的位置，偏离极小值点。

而学习率过低的情况时，模型的收敛速度较慢，效率较差。综合考虑下，选择$0.001$的学习率较好。

#### 隐藏层节点

在测试了多个隐藏层配置后，我发现其对收敛速度和过程中的震荡没有明显的影响，然而节点数更大的隐藏层，可以使得最后收敛的loss更小，得到更好的收敛结果。故选择3个512节点的隐藏层。

#### 采样点数

当采样点数量较少时，其loss的震荡较大，这是因为采样点少的时候，一次迭代只能影响较少的数据点附近的拟合效果，从而对其它位置的拟合效果较差，也就会使得loss的震荡较宽。当采样点数量较大时，一次迭代所涉及的数据点更多了，也就会使得迭代效果更好，loss的震荡较窄。故选择4096的采样点数。

### 误差分析

![看起来不太对啊](./data/residual.png)

从残差的冷热图上可以看到，整体的残差分布较为均匀，但在角点处的残差仍然较大，说明模型在正方体的棱边附近的拟合效果较差，这是一个比较难处理的问题。就算给边界附近添加了较大的权重，仍然无法消除这一部分残差，模型在这些刚性区域较难学习。不过这似乎是一个堆叠num_epochs之后可以一定程度上缓解的问题，但也没法完全消除。

## 实验结论

（AI总结前文）

本次基于PINN（Physics-Informed Neural Networks）方法求解立方体区域内泊松方程的实验表明：

1. **边界条件处理**

   通过构造形式 $\phi(x, y, z)=(1-x^2)(1-y^2)(1-z^2)u(x, y, z)$，模型天然满足零边界条件，无需额外损失项约束边界，简化了训练流程，提高了训练稳定性。

2. **模型结构与激活函数**

   使用3层隐藏层，每层512个节点的全连接网络，并采用`tanh`激活函数，使得网络能够顺利计算二阶导数，保证了PDE项的精确计算。相比ReLU等非光滑激活函数，`tanh`在求解泊松方程中表现更佳。

3. **训练策略**

   利用`accelerate`库在多GPU环境下进行训练，实现分布式数据并行（DDP），充分提升了采样效率和算力利用率。5张GPU训练时，等效采样点数提升了5倍，显存占用相对低（约600MiB/GPU），显著加快了训练速度。

4. **超参数选择**

   * **学习率**：过大（如0.005）会导致收敛不稳定，过小则训练缓慢；综合考虑选择0.001较优。
   * **隐藏层节点数**：增加节点数可进一步降低最终收敛loss，但对震荡幅度影响不大。
   * **采样点数量**：较大的采样点数（4096）可减少loss震荡，使迭代效果更平滑，提升整体拟合精度。

5. **训练结果与误差分析**

   模型在绝大多数区域的拟合效果良好，但在立方体的角点及棱边附近残差仍较大，说明网络在这些“刚性区域”较难收敛。这是PINN在边界条件复杂或几何尖角区域常见的挑战，可能需要通过增加迭代次数或采用局部加权策略进一步缓解。

6. **总结**

   本实验验证了PINN在三维泊松方程求解中的有效性和可行性，尤其在规则边界条件下，通过边界条件嵌入技巧可以简化训练；同时，多GPU分布式训练显著提高了计算效率。尽管角点残差问题仍存在，但整体精度和训练稳定性达到了预期目标，为后续更复杂几何或高维PDE问题的PINN求解提供了可行的实践经验。
